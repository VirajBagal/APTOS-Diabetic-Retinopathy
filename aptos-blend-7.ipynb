{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aptos-zoom-rs-1', 'aptos-effb5-changing-lr', 'aptos2019-blindness-detection']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "# tqdm.pandas()\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import model_from_json\n",
    "from keras import layers\n",
    "from keras.applications import DenseNet121\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "from PIL import Image\n",
    "print(os.listdir('../input'))\n",
    "\n",
    "im_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "''' ## Credits\n",
    "All credits are due to https://github.com/qubvel/efficientnet\n",
    "Thanks so much for your contribution!\n",
    "\n",
    "## Usage:\n",
    "Adding this utility script to your kernel, and you will be able to \n",
    "use all models just like standard Keras pretrained model. For details see\n",
    "https://www.kaggle.com/c/aptos2019-blindness-detection/discussion/100186\n",
    "\n",
    "## Pretrained Weights\n",
    "https://www.kaggle.com/ratthachat/efficientnet-keras-weights-b0b5/\n",
    "'''\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import keras.layers as KL\n",
    "from keras.initializers import Initializer\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "import os\n",
    "import re\n",
    "import collections\n",
    "import math\n",
    "import six\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "import keras.models as KM\n",
    "from keras.utils import get_file\n",
    "\n",
    "MEAN_RGB = [0.485 * 255, 0.456 * 255, 0.406 * 255]\n",
    "STDDEV_RGB = [0.229 * 255, 0.224 * 255, 0.225 * 255]\n",
    "\n",
    "MAP_INTERPOLATION_TO_ORDER = {\n",
    "    \"nearest\": 0,\n",
    "    \"bilinear\": 1,\n",
    "    \"biquadratic\": 2,\n",
    "    \"bicubic\": 3,\n",
    "}\n",
    "\n",
    "\n",
    "def center_crop_and_resize(image, image_size, crop_padding=32, interpolation=\"bicubic\"):\n",
    "    assert image.ndim in {2, 3}\n",
    "    assert interpolation in MAP_INTERPOLATION_TO_ORDER.keys()\n",
    "\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    padded_center_crop_size = int(\n",
    "        (image_size / (image_size + crop_padding)) * min(h, w)\n",
    "    )\n",
    "    offset_height = ((h - padded_center_crop_size) + 1) // 2\n",
    "    offset_width = ((w - padded_center_crop_size) + 1) // 2\n",
    "\n",
    "    image_crop = image[\n",
    "        offset_height : padded_center_crop_size + offset_height,\n",
    "        offset_width : padded_center_crop_size + offset_width,\n",
    "    ]\n",
    "    resized_image = resize(\n",
    "        image_crop,\n",
    "        (image_size, image_size),\n",
    "        order=MAP_INTERPOLATION_TO_ORDER[interpolation],\n",
    "        preserve_range=True,\n",
    "    )\n",
    "\n",
    "    return resized_image\n",
    "\n",
    "\n",
    "def preprocess_input(x):\n",
    "    assert x.ndim in (3, 4)\n",
    "    assert x.shape[-1] == 3\n",
    "\n",
    "    x = x - np.array(MEAN_RGB)\n",
    "    x = x / np.array(STDDEV_RGB)\n",
    "\n",
    "    return x\n",
    "\n",
    "class EfficientConv2DKernelInitializer(Initializer):\n",
    "    \"\"\"Initialization for convolutional kernels.\n",
    "    The main difference with tf.variance_scaling_initializer is that\n",
    "    tf.variance_scaling_initializer uses a truncated normal with an uncorrected\n",
    "    standard deviation, whereas here we use a normal distribution. Similarly,\n",
    "    tf.contrib.layers.variance_scaling_initializer uses a truncated normal with\n",
    "    a corrected standard deviation.\n",
    "    Args:\n",
    "      shape: shape of variable\n",
    "      dtype: dtype of variable\n",
    "      partition_info: unused\n",
    "    Returns:\n",
    "      an initialization for the variable\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, shape, dtype=K.floatx(), **kwargs):\n",
    "        kernel_height, kernel_width, _, out_filters = shape\n",
    "        fan_out = int(kernel_height * kernel_width * out_filters)\n",
    "        return tf.random_normal(\n",
    "            shape, mean=0.0, stddev=np.sqrt(2.0 / fan_out), dtype=dtype\n",
    "        )\n",
    "\n",
    "\n",
    "class EfficientDenseKernelInitializer(Initializer):\n",
    "    \"\"\"Initialization for dense kernels.\n",
    "    This initialization is equal to\n",
    "      tf.variance_scaling_initializer(scale=1.0/3.0, mode='fan_out',\n",
    "                                      distribution='uniform').\n",
    "    It is written out explicitly here for clarity.\n",
    "    Args:\n",
    "      shape: shape of variable\n",
    "      dtype: dtype of variable\n",
    "    Returns:\n",
    "      an initialization for the variable\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, shape, dtype=K.floatx(), **kwargs):\n",
    "        \"\"\"Initialization for dense kernels.\n",
    "        This initialization is equal to\n",
    "          tf.variance_scaling_initializer(scale=1.0/3.0, mode='fan_out',\n",
    "                                          distribution='uniform').\n",
    "        It is written out explicitly here for clarity.\n",
    "        Args:\n",
    "          shape: shape of variable\n",
    "          dtype: dtype of variable\n",
    "        Returns:\n",
    "          an initialization for the variable\n",
    "        \"\"\"\n",
    "        init_range = 1.0 / np.sqrt(shape[1])\n",
    "        return tf.random_uniform(shape, -init_range, init_range, dtype=dtype)\n",
    "\n",
    "\n",
    "conv_kernel_initializer = EfficientConv2DKernelInitializer()\n",
    "dense_kernel_initializer = EfficientDenseKernelInitializer()\n",
    "\n",
    "\n",
    "get_custom_objects().update(\n",
    "    {\n",
    "        \"EfficientDenseKernelInitializer\": EfficientDenseKernelInitializer,\n",
    "        \"EfficientConv2DKernelInitializer\": EfficientConv2DKernelInitializer,\n",
    "    }\n",
    ")\n",
    "\n",
    "class Swish(KL.Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.swish(inputs)\n",
    "\n",
    "\n",
    "class DropConnect(KL.Layer):\n",
    "    def __init__(self, drop_connect_rate=0.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.drop_connect_rate = drop_connect_rate\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        def drop_connect():\n",
    "            keep_prob = 1.0 - self.drop_connect_rate\n",
    "\n",
    "            # Compute drop_connect tensor\n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            random_tensor = keep_prob\n",
    "            random_tensor += tf.random_uniform(\n",
    "                [batch_size, 1, 1, 1], dtype=inputs.dtype\n",
    "            )\n",
    "            binary_tensor = tf.floor(random_tensor)\n",
    "            output = tf.div(inputs, keep_prob) * binary_tensor\n",
    "            return output\n",
    "\n",
    "        return K.in_train_phase(drop_connect, inputs, training=training)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config[\"drop_connect_rate\"] = self.drop_connect_rate\n",
    "        return config\n",
    "\n",
    "\n",
    "get_custom_objects().update({\"DropConnect\": DropConnect, \"Swish\": Swish})\n",
    "\n",
    "\n",
    "IMAGENET_WEIGHTS = {\n",
    "    \"efficientnet-b0\": {\n",
    "        \"name\": \"efficientnet-b0_imagenet_1000.h5\",\n",
    "        \"url\": \"https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000.h5\",\n",
    "        \"md5\": \"bca04d16b1b8a7c607b1152fe9261af7\",\n",
    "    },\n",
    "    \"efficientnet-b0-notop\": {\n",
    "        \"name\": \"efficientnet-b0_imagenet_1000_notop.h5\",\n",
    "        \"url\": \"https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\",\n",
    "        \"md5\": \"45d2f3b6330c2401ef66da3961cad769\",\n",
    "    },\n",
    "    \"efficientnet-b1\": {\n",
    "        \"name\": \"efficientnet-b1_imagenet_1000.h5\",\n",
    "        \"url\": \"https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b1_imagenet_1000.h5\",\n",
    "        \"md5\": \"bd4a2b82f6f6bada74fc754553c464fc\",\n",
    "    },\n",
    "    \"efficientnet-b1-notop\": {\n",
    "        \"name\": \"efficientnet-b1_imagenet_1000_notop.h5\",\n",
    "        \"url\": \"https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b1_imagenet_1000_notop.h5\",\n",
    "        \"md5\": \"884aed586c2d8ca8dd15a605ec42f564\",\n",
    "    },\n",
    "    \"efficientnet-b2\": {\n",
    "        \"name\": \"efficientnet-b2_imagenet_1000.h5\",\n",
    "        \"url\": \"https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b2_imagenet_1000.h5\",\n",
    "        \"md5\": \"45b28b26f15958bac270ab527a376999\",\n",
    "    },\n",
    "    \"efficientnet-b2-notop\": {\n",
    "        \"name\": \"efficientnet-b2_imagenet_1000_notop.h5\",\n",
    "        \"url\": \"https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b2_imagenet_1000_notop.h5\",\n",
    "        \"md5\": \"42fb9f2d9243d461d62b4555d3a53b7b\",\n",
    "    },\n",
    "    \"efficientnet-b3\": {\n",
    "        \"name\": \"efficientnet-b3_imagenet_1000.h5\",\n",
    "        \"url\": \"https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b3_imagenet_1000.h5\",\n",
    "        \"md5\": \"decd2c8a23971734f9d3f6b4053bf424\",\n",
    "    },\n",
    "    \"efficientnet-b3-notop\": {\n",
    "        \"name\": \"efficientnet-b3_imagenet_1000_notop.h5\",\n",
    "        \"url\": \"https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b3_imagenet_1000_notop.h5\",\n",
    "        \"md5\": \"1f7d9a8c2469d2e3d3b97680d45df1e1\",\n",
    "    },\n",
    "    \"efficientnet-b4\": {\n",
    "        \"name\": \"efficientnet-b4_imagenet_1000.h5\",\n",
    "        \"url\": \"https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b4_imagenet_1000.h5\",\n",
    "        \"md5\": \"01df77157a86609530aeb4f1f9527949\",\n",
    "    },\n",
    "    \"efficientnet-b4-notop\": {\n",
    "        \"name\": \"efficientnet-b4_imagenet_1000_notop.h5\",\n",
    "        \"url\": \"https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b4_imagenet_1000_notop.h5\",\n",
    "        \"md5\": \"e7c3b780f050f8f49c800f23703f285c\",\n",
    "    },\n",
    "    \"efficientnet-b5\": {\n",
    "        \"name\": \"efficientnet-b5_imagenet_1000.h5\",\n",
    "        \"url\": \"https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b5_imagenet_1000.h5\",\n",
    "        \"md5\": \"c31311a1a38b5111e14457145fccdf32\",\n",
    "    },\n",
    "    \"efficientnet-b5-notop\": {\n",
    "        \"name\": \"efficientnet-b5_imagenet_1000_notop.h5\",\n",
    "        \"url\": \"https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b5_imagenet_1000_notop.h5\",\n",
    "        \"md5\": \"a09b36129b41196e0bb659fd84fbdd5f\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "GlobalParams = collections.namedtuple(\n",
    "    \"GlobalParams\",\n",
    "    [\n",
    "        \"batch_norm_momentum\",\n",
    "        \"batch_norm_epsilon\",\n",
    "        \"dropout_rate\",\n",
    "        \"data_format\",\n",
    "        \"num_classes\",\n",
    "        \"width_coefficient\",\n",
    "        \"depth_coefficient\",\n",
    "        \"depth_divisor\",\n",
    "        \"min_depth\",\n",
    "        \"drop_connect_rate\",\n",
    "    ],\n",
    ")\n",
    "GlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\n",
    "\n",
    "BlockArgs = collections.namedtuple(\n",
    "    \"BlockArgs\",\n",
    "    [\n",
    "        \"kernel_size\",\n",
    "        \"num_repeat\",\n",
    "        \"input_filters\",\n",
    "        \"output_filters\",\n",
    "        \"expand_ratio\",\n",
    "        \"id_skip\",\n",
    "        \"strides\",\n",
    "        \"se_ratio\",\n",
    "    ],\n",
    ")\n",
    "# defaults will be a public argument for namedtuple in Python 3.7\n",
    "# https://docs.python.org/3/library/collections.html#collections.namedtuple\n",
    "BlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n",
    "\n",
    "\n",
    "def efficientnet_params(model_name):\n",
    "    \"\"\"Get efficientnet params based on model name.\"\"\"\n",
    "    params_dict = {\n",
    "        # (width_coefficient, depth_coefficient, resolution, dropout_rate)\n",
    "        \"efficientnet-b0\": (1.0, 1.0, 224, 0.2),\n",
    "        \"efficientnet-b1\": (1.0, 1.1, 240, 0.2),\n",
    "        \"efficientnet-b2\": (1.1, 1.2, 260, 0.3),\n",
    "        \"efficientnet-b3\": (1.2, 1.4, 300, 0.3),\n",
    "        \"efficientnet-b4\": (1.4, 1.8, 380, 0.4),\n",
    "        \"efficientnet-b5\": (1.6, 2.2, 456, 0.4),\n",
    "        \"efficientnet-b6\": (1.8, 2.6, 528, 0.5),\n",
    "        \"efficientnet-b7\": (2.0, 3.1, 600, 0.5),\n",
    "    }\n",
    "    return params_dict[model_name]\n",
    "\n",
    "\n",
    "class BlockDecoder(object):\n",
    "    \"\"\"Block Decoder for readability.\"\"\"\n",
    "\n",
    "    def _decode_block_string(self, block_string):\n",
    "        \"\"\"Gets a block through a string notation of arguments.\"\"\"\n",
    "        assert isinstance(block_string, str)\n",
    "        ops = block_string.split(\"_\")\n",
    "        options = {}\n",
    "        for op in ops:\n",
    "            splits = re.split(r\"(\\d.*)\", op)\n",
    "            if len(splits) >= 2:\n",
    "                key, value = splits[:2]\n",
    "                options[key] = value\n",
    "\n",
    "        if \"s\" not in options or len(options[\"s\"]) != 2:\n",
    "            raise ValueError(\"Strides options should be a pair of integers.\")\n",
    "\n",
    "        return BlockArgs(\n",
    "            kernel_size=int(options[\"k\"]),\n",
    "            num_repeat=int(options[\"r\"]),\n",
    "            input_filters=int(options[\"i\"]),\n",
    "            output_filters=int(options[\"o\"]),\n",
    "            expand_ratio=int(options[\"e\"]),\n",
    "            id_skip=(\"noskip\" not in block_string),\n",
    "            se_ratio=float(options[\"se\"]) if \"se\" in options else None,\n",
    "            strides=[int(options[\"s\"][0]), int(options[\"s\"][1])],\n",
    "        )\n",
    "\n",
    "    def _encode_block_string(self, block):\n",
    "        \"\"\"Encodes a block to a string.\"\"\"\n",
    "        args = [\n",
    "            \"r%d\" % block.num_repeat,\n",
    "            \"k%d\" % block.kernel_size,\n",
    "            \"s%d%d\" % (block.strides[0], block.strides[1]),\n",
    "            \"e%s\" % block.expand_ratio,\n",
    "            \"i%d\" % block.input_filters,\n",
    "            \"o%d\" % block.output_filters,\n",
    "        ]\n",
    "        if block.se_ratio > 0 and block.se_ratio <= 1:\n",
    "            args.append(\"se%s\" % block.se_ratio)\n",
    "        if block.id_skip is False:\n",
    "            args.append(\"noskip\")\n",
    "        return \"_\".join(args)\n",
    "\n",
    "    def decode(self, string_list):\n",
    "        \"\"\"Decodes a list of string notations to specify blocks inside the network.\n",
    "    Args:\n",
    "      string_list: a list of strings, each string is a notation of block.\n",
    "    Returns:\n",
    "      A list of namedtuples to represent blocks arguments.\n",
    "    \"\"\"\n",
    "        assert isinstance(string_list, list)\n",
    "        blocks_args = []\n",
    "        for block_string in string_list:\n",
    "            blocks_args.append(self._decode_block_string(block_string))\n",
    "        return blocks_args\n",
    "\n",
    "    def encode(self, blocks_args):\n",
    "        \"\"\"Encodes a list of Blocks to a list of strings.\n",
    "    Args:\n",
    "      blocks_args: A list of namedtuples to represent blocks arguments.\n",
    "    Returns:\n",
    "      a list of strings, each string is a notation of block.\n",
    "    \"\"\"\n",
    "        block_strings = []\n",
    "        for block in blocks_args:\n",
    "            block_strings.append(self._encode_block_string(block))\n",
    "        return block_strings\n",
    "\n",
    "\n",
    "def efficientnet(\n",
    "    width_coefficient=None,\n",
    "    depth_coefficient=None,\n",
    "    dropout_rate=0.2,\n",
    "    drop_connect_rate=0.2,\n",
    "):\n",
    "    \"\"\"Creates a efficientnet model.\"\"\"\n",
    "    blocks_args = [\n",
    "        \"r1_k3_s11_e1_i32_o16_se0.25\",\n",
    "        \"r2_k3_s22_e6_i16_o24_se0.25\",\n",
    "        \"r2_k5_s22_e6_i24_o40_se0.25\",\n",
    "        \"r3_k3_s22_e6_i40_o80_se0.25\",\n",
    "        \"r3_k5_s11_e6_i80_o112_se0.25\",\n",
    "        \"r4_k5_s22_e6_i112_o192_se0.25\",\n",
    "        \"r1_k3_s11_e6_i192_o320_se0.25\",\n",
    "    ]\n",
    "    global_params = GlobalParams(\n",
    "        batch_norm_momentum=0.99,\n",
    "        batch_norm_epsilon=1e-3,\n",
    "        dropout_rate=dropout_rate,\n",
    "        drop_connect_rate=drop_connect_rate,\n",
    "        data_format=\"channels_last\",\n",
    "        num_classes=1000,\n",
    "        width_coefficient=width_coefficient,\n",
    "        depth_coefficient=depth_coefficient,\n",
    "        depth_divisor=8,\n",
    "        min_depth=None,\n",
    "    )\n",
    "    decoder = BlockDecoder()\n",
    "    return decoder.decode(blocks_args), global_params\n",
    "\n",
    "\n",
    "def get_model_params(model_name, override_params=None):\n",
    "    \"\"\"Get the block args and global params for a given model.\"\"\"\n",
    "    if model_name.startswith(\"efficientnet\"):\n",
    "        width_coefficient, depth_coefficient, input_shape, dropout_rate = efficientnet_params(\n",
    "            model_name\n",
    "        )\n",
    "        blocks_args, global_params = efficientnet(\n",
    "            width_coefficient, depth_coefficient, dropout_rate\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(\"model name is not pre-defined: %s\" % model_name)\n",
    "\n",
    "    if override_params:\n",
    "        # ValueError will be raised here if override_params has fields not included\n",
    "        # in global_params.\n",
    "        global_params = global_params._replace(**override_params)\n",
    "\n",
    "    # print('global_params= %s', global_params)\n",
    "    # print('blocks_args= %s', blocks_args)\n",
    "    return blocks_args, global_params, input_shape\n",
    "\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    \"EfficientNet\",\n",
    "    \"EfficientNetB0\",\n",
    "    \"EfficientNetB1\",\n",
    "    \"EfficientNetB2\",\n",
    "    \"EfficientNetB3\",\n",
    "    \"EfficientNetB4\",\n",
    "    \"EfficientNetB5\",\n",
    "    \"EfficientNetB6\",\n",
    "    \"EfficientNetB7\",\n",
    "]\n",
    "\n",
    "\n",
    "def round_filters(filters, global_params):\n",
    "    \"\"\"Round number of filters based on depth multiplier.\"\"\"\n",
    "    orig_f = filters\n",
    "    multiplier = global_params.width_coefficient\n",
    "    divisor = global_params.depth_divisor\n",
    "    min_depth = global_params.min_depth\n",
    "    if not multiplier:\n",
    "        return filters\n",
    "\n",
    "    filters *= multiplier\n",
    "    min_depth = min_depth or divisor\n",
    "    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_filters < 0.9 * filters:\n",
    "        new_filters += divisor\n",
    "    # print('round_filter input={} output={}'.format(orig_f, new_filters))\n",
    "    return int(new_filters)\n",
    "\n",
    "\n",
    "def round_repeats(repeats, global_params):\n",
    "    \"\"\"Round number of filters based on depth multiplier.\"\"\"\n",
    "    multiplier = global_params.depth_coefficient\n",
    "    if not multiplier:\n",
    "        return repeats\n",
    "    return int(math.ceil(multiplier * repeats))\n",
    "\n",
    "\n",
    "def SEBlock(block_args, global_params):\n",
    "    num_reduced_filters = max(1, int(block_args.input_filters * block_args.se_ratio))\n",
    "    filters = block_args.input_filters * block_args.expand_ratio\n",
    "    if global_params.data_format == \"channels_first\":\n",
    "        channel_axis = 1\n",
    "        spatial_dims = [2, 3]\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "        spatial_dims = [1, 2]\n",
    "\n",
    "    def block(inputs):\n",
    "        x = inputs\n",
    "        x = KL.Lambda(lambda a: K.mean(a, axis=spatial_dims, keepdims=True))(x)\n",
    "        x = KL.Conv2D(\n",
    "            num_reduced_filters,\n",
    "            kernel_size=[1, 1],\n",
    "            strides=[1, 1],\n",
    "            kernel_initializer=conv_kernel_initializer,\n",
    "            padding=\"same\",\n",
    "            use_bias=True,\n",
    "        )(x)\n",
    "        x = Swish()(x)\n",
    "        # Excite\n",
    "        x = KL.Conv2D(\n",
    "            filters,\n",
    "            kernel_size=[1, 1],\n",
    "            strides=[1, 1],\n",
    "            kernel_initializer=conv_kernel_initializer,\n",
    "            padding=\"same\",\n",
    "            use_bias=True,\n",
    "        )(x)\n",
    "        x = KL.Activation(\"sigmoid\")(x)\n",
    "        out = KL.Multiply()([x, inputs])\n",
    "        return out\n",
    "\n",
    "    return block\n",
    "\n",
    "\n",
    "def MBConvBlock(block_args, global_params, drop_connect_rate=None):\n",
    "    batch_norm_momentum = global_params.batch_norm_momentum\n",
    "    batch_norm_epsilon = global_params.batch_norm_epsilon\n",
    "\n",
    "    if global_params.data_format == \"channels_first\":\n",
    "        channel_axis = 1\n",
    "        spatial_dims = [2, 3]\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "        spatial_dims = [1, 2]\n",
    "\n",
    "    has_se = (\n",
    "        (block_args.se_ratio is not None)\n",
    "        and (block_args.se_ratio > 0)\n",
    "        and (block_args.se_ratio <= 1)\n",
    "    )\n",
    "\n",
    "    filters = block_args.input_filters * block_args.expand_ratio\n",
    "    kernel_size = block_args.kernel_size\n",
    "\n",
    "    def block(inputs):\n",
    "\n",
    "        if block_args.expand_ratio != 1:\n",
    "            x = KL.Conv2D(\n",
    "                filters,\n",
    "                kernel_size=[1, 1],\n",
    "                strides=[1, 1],\n",
    "                kernel_initializer=conv_kernel_initializer,\n",
    "                padding=\"same\",\n",
    "                use_bias=False,\n",
    "            )(inputs)\n",
    "            x = KL.BatchNormalization(\n",
    "                axis=channel_axis,\n",
    "                momentum=batch_norm_momentum,\n",
    "                epsilon=batch_norm_epsilon,\n",
    "            )(x)\n",
    "            x = Swish()(x)\n",
    "        else:\n",
    "            x = inputs\n",
    "\n",
    "        x = KL.DepthwiseConv2D(\n",
    "            [kernel_size, kernel_size],\n",
    "            strides=block_args.strides,\n",
    "            depthwise_initializer=conv_kernel_initializer,\n",
    "            padding=\"same\",\n",
    "            use_bias=False,\n",
    "        )(x)\n",
    "        x = KL.BatchNormalization(\n",
    "            axis=channel_axis, momentum=batch_norm_momentum, epsilon=batch_norm_epsilon\n",
    "        )(x)\n",
    "        x = Swish()(x)\n",
    "\n",
    "        if has_se:\n",
    "            x = SEBlock(block_args, global_params)(x)\n",
    "\n",
    "        # output phase\n",
    "\n",
    "        x = KL.Conv2D(\n",
    "            block_args.output_filters,\n",
    "            kernel_size=[1, 1],\n",
    "            strides=[1, 1],\n",
    "            kernel_initializer=conv_kernel_initializer,\n",
    "            padding=\"same\",\n",
    "            use_bias=False,\n",
    "        )(x)\n",
    "        x = KL.BatchNormalization(\n",
    "            axis=channel_axis, momentum=batch_norm_momentum, epsilon=batch_norm_epsilon\n",
    "        )(x)\n",
    "\n",
    "        if block_args.id_skip:\n",
    "            if (\n",
    "                all(s == 1 for s in block_args.strides)\n",
    "                and block_args.input_filters == block_args.output_filters\n",
    "            ):\n",
    "                # only apply drop_connect if skip presents.\n",
    "                if drop_connect_rate:\n",
    "                    x = DropConnect(drop_connect_rate)(x)\n",
    "                x = KL.Add()([x, inputs])\n",
    "        return x\n",
    "\n",
    "    return block\n",
    "\n",
    "\n",
    "def EfficientNet(\n",
    "    input_shape, block_args_list, global_params, input_tensor=None, include_top=True, pooling=None\n",
    "):\n",
    "    batch_norm_momentum = global_params.batch_norm_momentum\n",
    "    batch_norm_epsilon = global_params.batch_norm_epsilon\n",
    "    if global_params.data_format == \"channels_first\":\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "\n",
    "    # Stem part\n",
    "    if input_tensor is None:\n",
    "        inputs = KL.Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            inputs = KL.Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            inputs = input_tensor\n",
    "    x = inputs\n",
    "    x = KL.Conv2D(\n",
    "        filters=round_filters(32, global_params),\n",
    "        kernel_size=[3, 3],\n",
    "        strides=[2, 2],\n",
    "        kernel_initializer=conv_kernel_initializer,\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "    )(x)\n",
    "    x = KL.BatchNormalization(\n",
    "        axis=channel_axis, momentum=batch_norm_momentum, epsilon=batch_norm_epsilon\n",
    "    )(x)\n",
    "    x = Swish()(x)\n",
    "\n",
    "    # Blocks part\n",
    "    block_idx = 1\n",
    "    n_blocks = sum([block_args.num_repeat for block_args in block_args_list])\n",
    "    drop_rate = global_params.drop_connect_rate or 0\n",
    "    drop_rate_dx = drop_rate / n_blocks\n",
    "\n",
    "    for block_args in block_args_list:\n",
    "        assert block_args.num_repeat > 0\n",
    "        # Update block input and output filters based on depth multiplier.\n",
    "        block_args = block_args._replace(\n",
    "            input_filters=round_filters(block_args.input_filters, global_params),\n",
    "            output_filters=round_filters(block_args.output_filters, global_params),\n",
    "            num_repeat=round_repeats(block_args.num_repeat, global_params),\n",
    "        )\n",
    "\n",
    "        # The first block needs to take care of stride and filter size increase.\n",
    "        x = MBConvBlock(\n",
    "            block_args, global_params, drop_connect_rate=drop_rate_dx * block_idx\n",
    "        )(x)\n",
    "        block_idx += 1\n",
    "\n",
    "        if block_args.num_repeat > 1:\n",
    "            block_args = block_args._replace(\n",
    "                input_filters=block_args.output_filters, strides=[1, 1]\n",
    "            )\n",
    "\n",
    "        for _ in xrange(block_args.num_repeat - 1):\n",
    "            x = MBConvBlock(\n",
    "                block_args, global_params, drop_connect_rate=drop_rate_dx * block_idx\n",
    "            )(x)\n",
    "            block_idx += 1\n",
    "\n",
    "    # Head part\n",
    "    x = KL.Conv2D(\n",
    "        filters=round_filters(1280, global_params),\n",
    "        kernel_size=[1, 1],\n",
    "        strides=[1, 1],\n",
    "        kernel_initializer=conv_kernel_initializer,\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "    )(x)\n",
    "    x = KL.BatchNormalization(\n",
    "        axis=channel_axis, momentum=batch_norm_momentum, epsilon=batch_norm_epsilon\n",
    "    )(x)\n",
    "    x = Swish()(x)\n",
    "\n",
    "    if include_top:\n",
    "        x = KL.GlobalAveragePooling2D(data_format=global_params.data_format)(x)\n",
    "        if global_params.dropout_rate > 0:\n",
    "            x = KL.Dropout(global_params.dropout_rate)(x)\n",
    "        x = KL.Dense(\n",
    "            global_params.num_classes, kernel_initializer=dense_kernel_initializer\n",
    "        )(x)\n",
    "        x = KL.Activation(\"softmax\")(x)\n",
    "    else:\n",
    "        if pooling == \"avg\":\n",
    "            x = KL.GlobalAveragePooling2D(data_format=global_params.data_format)(x)\n",
    "        elif pooling == \"max\":\n",
    "            x = KL.GlobalMaxPooling2D(data_format=global_params.data_format)(x)\n",
    "\n",
    "    outputs = x\n",
    "    model = KM.Model(inputs, outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def _get_model_by_name(\n",
    "    model_name, \n",
    "    input_shape=None, \n",
    "    input_tensor=None, \n",
    "    include_top=True, \n",
    "    weights=None, \n",
    "    classes=1000, \n",
    "    pooling=None\n",
    "):\n",
    "    \"\"\"Re-Implementation of EfficientNet for Keras\n",
    "    Reference:\n",
    "        https://arxiv.org/abs/1807.11626\n",
    "    Args:\n",
    "        input_shape: optional, if ``None`` default_input_shape is used\n",
    "            EfficientNetB0 - (224, 224, 3)\n",
    "            EfficientNetB1 - (240, 240, 3)\n",
    "            EfficientNetB2 - (260, 260, 3)\n",
    "            EfficientNetB3 - (300, 300, 3)\n",
    "            EfficientNetB4 - (380, 380, 3)\n",
    "            EfficientNetB5 - (456, 456, 3)\n",
    "            EfficientNetB6 - (528, 528, 3)\n",
    "            EfficientNetB7 - (600, 600, 3)\n",
    "        input_tensor: optional, if ``None`` default_input_tensor is used\n",
    "        include_top: whether to include the fully-connected\n",
    "            layer at the top of the network.\n",
    "        weights: one of `None` (random initialization),\n",
    "              'imagenet' (pre-training on ImageNet).\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "        pooling: optional [None, 'avg', 'max'], if ``include_top=False``\n",
    "            add global pooling on top of the network\n",
    "            - avg: GlobalAveragePooling2D\n",
    "            - max: GlobalMaxPooling2D\n",
    "    Returns:\n",
    "        A Keras model instance.\n",
    "    \"\"\"\n",
    "    if weights not in {None, \"imagenet\"}:\n",
    "        raise ValueError('Parameter `weights` should be one of [None, \"imagenet\"]')\n",
    "\n",
    "    if weights == \"imagenet\" and model_name not in IMAGENET_WEIGHTS:\n",
    "        raise ValueError(\n",
    "            \"There are not pretrained weights for {} model.\".format(model_name)\n",
    "        )\n",
    "\n",
    "    if weights == \"imagenet\" and include_top and classes != 1000:\n",
    "        raise ValueError(\n",
    "            \"If using `weights` and `include_top`\" \" `classes` should be 1000\"\n",
    "        )\n",
    "\n",
    "    block_agrs_list, global_params, default_input_shape = get_model_params(\n",
    "        model_name, override_params={\"num_classes\": classes}\n",
    "    )\n",
    "\n",
    "    if input_shape is None:\n",
    "        input_shape = (default_input_shape, default_input_shape, 3)\n",
    "        \n",
    "    model = EfficientNet(\n",
    "        input_shape,\n",
    "        block_agrs_list,\n",
    "        global_params,\n",
    "        input_tensor=input_tensor,\n",
    "        include_top=include_top,\n",
    "        pooling=pooling,\n",
    "    )\n",
    "\n",
    "    model.name = model_name\n",
    "\n",
    "    if weights:\n",
    "        if not include_top:\n",
    "            weights_name = model_name + \"-notop\"\n",
    "        else:\n",
    "            weights_name = model_name\n",
    "        weights = IMAGENET_WEIGHTS[weights_name]\n",
    "        weights_path = get_file(\n",
    "            weights[\"name\"],\n",
    "            weights[\"url\"],\n",
    "            cache_subdir=\"models\",\n",
    "            md5_hash=weights[\"md5\"],\n",
    "        )\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def EfficientNetB0(\n",
    "    include_top=True, input_shape=None, input_tensor=None, weights=None, classes=1000, pooling=None\n",
    "):\n",
    "    return _get_model_by_name(\n",
    "        \"efficientnet-b0\",\n",
    "        include_top=include_top,\n",
    "        input_shape=input_shape,\n",
    "        input_tensor=input_tensor,\n",
    "        weights=weights,\n",
    "        classes=classes,\n",
    "        pooling=pooling,\n",
    "    )\n",
    "\n",
    "\n",
    "def EfficientNetB1(\n",
    "    include_top=True, input_shape=None, input_tensor=None, weights=None, classes=1000, pooling=None\n",
    "):\n",
    "    return _get_model_by_name(\n",
    "        \"efficientnet-b1\",\n",
    "        include_top=include_top,\n",
    "        input_shape=input_shape,\n",
    "        input_tensor=input_tensor,\n",
    "        weights=weights,\n",
    "        classes=classes,\n",
    "        pooling=pooling,\n",
    "    )\n",
    "\n",
    "\n",
    "def EfficientNetB2(\n",
    "    include_top=True, input_shape=None, input_tensor=None, weights=None, classes=1000, pooling=None\n",
    "):\n",
    "    return _get_model_by_name(\n",
    "        \"efficientnet-b2\",\n",
    "        include_top=include_top,\n",
    "        input_shape=input_shape,\n",
    "        input_tensor=input_tensor,\n",
    "        weights=weights,\n",
    "        classes=classes,\n",
    "        pooling=pooling,\n",
    "    )\n",
    "\n",
    "\n",
    "def EfficientNetB3(\n",
    "    include_top=True, input_shape=None, input_tensor=None, weights=None, classes=1000, pooling=None\n",
    "):\n",
    "    return _get_model_by_name(\n",
    "        \"efficientnet-b3\",\n",
    "        include_top=include_top,\n",
    "        input_shape=input_shape,\n",
    "        input_tensor=input_tensor,\n",
    "        weights=weights,\n",
    "        classes=classes,\n",
    "        pooling=pooling,\n",
    "    )\n",
    "\n",
    "\n",
    "def EfficientNetB4(\n",
    "    include_top=True, input_shape=None, input_tensor=None, weights=None, classes=1000, pooling=None\n",
    "):\n",
    "    return _get_model_by_name(\n",
    "        \"efficientnet-b4\",\n",
    "        include_top=include_top,\n",
    "        input_shape=input_shape,\n",
    "        input_tensor=input_tensor,\n",
    "        weights=weights,\n",
    "        classes=classes,\n",
    "        pooling=pooling,\n",
    "    )\n",
    "\n",
    "\n",
    "def EfficientNetB5(\n",
    "    include_top=True, input_shape=None, input_tensor=None, weights=None, classes=1000, pooling=None\n",
    "):\n",
    "    return _get_model_by_name(\n",
    "        \"efficientnet-b5\",\n",
    "        include_top=include_top,\n",
    "        input_shape=input_shape,\n",
    "        input_tensor=input_tensor,\n",
    "        weights=weights,\n",
    "        classes=classes,\n",
    "        pooling=pooling,\n",
    "    )\n",
    "\n",
    "\n",
    "def EfficientNetB6(\n",
    "    include_top=True, input_shape=None, input_tensor=None, weights=None, classes=1000, pooling=None\n",
    "):\n",
    "    return _get_model_by_name(\n",
    "        \"efficientnet-b6\",\n",
    "        include_top=include_top,\n",
    "        input_shape=input_shape,\n",
    "        input_tensor=input_tensor,\n",
    "        weights=weights,\n",
    "        classes=classes,\n",
    "        pooling=pooling,\n",
    "    )\n",
    "\n",
    "\n",
    "def EfficientNetB7(\n",
    "    include_top=True, input_shape=None, input_tensor=None, weights=None, classes=1000, pooling=None\n",
    "):\n",
    "    return _get_model_by_name(\n",
    "        \"efficientnet-b7\",\n",
    "        include_top=include_top,\n",
    "        input_shape=input_shape,\n",
    "        input_tensor=input_tensor,\n",
    "        weights=weights,\n",
    "        classes=classes,\n",
    "        pooling=pooling,\n",
    "    )\n",
    "\n",
    "\n",
    "EfficientNetB0.__doc__ = _get_model_by_name.__doc__\n",
    "EfficientNetB1.__doc__ = _get_model_by_name.__doc__\n",
    "EfficientNetB2.__doc__ = _get_model_by_name.__doc__\n",
    "EfficientNetB3.__doc__ = _get_model_by_name.__doc__\n",
    "EfficientNetB4.__doc__ = _get_model_by_name.__doc__\n",
    "EfficientNetB5.__doc__ = _get_model_by_name.__doc__\n",
    "EfficientNetB6.__doc__ = _get_model_by_name.__doc__\n",
    "EfficientNetB7.__doc__ = _get_model_by_name.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1928, 1)\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\n",
    "\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Test Images\n",
    "\n",
    "---\n",
    "\n",
    "Notice that here we can only see the public dataset. Once we submit our results, the kernel will rerun on the overall test set (public + private) that is around 20 GB. That needs to be considered otherwise our kernel may not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop function: https://www.kaggle.com/ratthachat/aptos-updated-preprocessing-ben-s-cropping\n",
    "def crop_image1(img,tol=7):\n",
    "    # img is image data\n",
    "    # tol  is tolerance\n",
    "        \n",
    "    mask = img>tol\n",
    "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "\n",
    "def crop_image_from_gray(img,tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        \n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "        return img\n",
    "\n",
    "    \n",
    "def crop_image(image):\n",
    "    output = image.copy()\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    ret,gray = cv2.threshold(gray,10,255,cv2.THRESH_BINARY)\n",
    "    contours,hierarchy = cv2.findContours(gray,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        print('no contours!')\n",
    "        flag = 0\n",
    "        return image, flag\n",
    "    cnt = max(contours, key=cv2.contourArea)\n",
    "    ((x, y), r) = cv2.minEnclosingCircle(cnt)\n",
    "    x = int(x); y = int(y); r = int(r)\n",
    "    flag = 1\n",
    "    #print(x,y,r)\n",
    "    if r > 100:\n",
    "        return output[0 + (y-r)*int(r<y):-1 + (y+r+1)*int(r<y),0 + (x-r)*int(r<x):-1 + (x+r+1)*int(r<x)], flag\n",
    "    else:\n",
    "        print('none!')\n",
    "        flag = 0\n",
    "        return image,flag\n",
    "    \n",
    "def crop_image_from_gray(img,tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        \n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "        return img\n",
    "    \n",
    "def preprocess_image(image_path, desired_size=256):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = crop_image_from_gray(img)\n",
    "    img = cv2.resize(img, (desired_size,desired_size))\n",
    "    img = cv2.addWeighted(img,4,cv2.GaussianBlur(img, (0,0), 10) ,-4 ,128)\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - EfficientNetB5\n",
    "\n",
    "--- \n",
    "\n",
    "Here we need to replicate the architecture we used during the training phase. The difference is that we are going to load the weights we have previously found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file=open('../input/aptos-effb5-changing-lr/model.json','r')\n",
    "json_model=json_file.read()\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_from_json(json_model)\n",
    "model.load_weights('../input/aptos-effb5-changing-lr/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred1=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1928/1928 [03:20<00:00,  8.56it/s]\n"
     ]
    }
   ],
   "source": [
    "for i,img_code in enumerate(tqdm(test_df['id_code'])):\n",
    "    img=preprocess_image(f'../input/aptos2019-blindness-detection/test_images/{img_code}.png')\n",
    "    pred=model.predict(img[np.newaxis]/255)\n",
    "    test_pred1.append(pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred1=np.concatenate(test_pred1,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file=open('../input/aptos-zoom-rs-1/model.json','r')\n",
    "json_model=json_file.read()\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_from_json(json_model)\n",
    "model.load_weights('../input/aptos-zoom-rs-1/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1928/1928 [03:25<00:00,  9.37it/s]\n"
     ]
    }
   ],
   "source": [
    "for i,img_code in enumerate(tqdm(test_df['id_code'])):\n",
    "    img=preprocess_image(f'../input/aptos2019-blindness-detection/test_images/{img_code}.png')\n",
    "    pred=model.predict(img[np.newaxis]/255)\n",
    "    test_pred3.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred3=np.concatenate(test_pred3,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = 0.4*test_pred1  + 0.6*test_pred3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = test_pred > 0.5\n",
    "test_pred = test_pred.astype(int).sum(axis=1) - 1\n",
    "test_pred = pd.Series(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id_code  diagnosis\n",
      "0  0005cfc8afb6          2\n",
      "1  003f0afdcd15          3\n",
      "2  006efc72b638          3\n",
      "3  00836aaacf06          2\n",
      "4  009245722fa4          2\n"
     ]
    }
   ],
   "source": [
    "test_df['diagnosis'] = test_pred\n",
    "test_df.to_csv('submission.csv',index=False)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction distribution:\n",
      "2    56.379668\n",
      "0    18.568465\n",
      "3    14.574689\n",
      "1     7.105809\n",
      "4     3.371369\n",
      "Name: diagnosis, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEj5JREFUeJzt3X+s3XV9x/HnWwqi3NlWcHek7VYWGzcGU+lNrTMxt9YoPwwlGSQYJoVgmkymbLhINdnI3MwwGaKyRdMJsWydhSFZO8Q5VrgzJqOTIlKwOiproNC1aku1UueY7/1xPsyby+2995zvvd9zyuf5SG7u9/v9fL7n8z6f3m9f9/s953xvZCaSpPq8rN8FSJL6wwCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVWpevwuYymmnnZZLly7tef8f//jHnHLKKbNX0Cyxru5YV3esqzsvxbp27Njx/cx8zbQdM3Ngv5YvX55N3H///Y32nyvW1R3r6o51deelWBfwYM7g/1gvAUlSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUG+lYQ0iDb+fRhrlj/pdbH3XPDBa2PqZcmzwAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVKlpAyAibo2IAxHx6Lhtr46IeyPi8fJ9YdkeEfHpiNgdEY9ExDnj9llb+j8eEWvn5ulIkmZqJmcAnwfOnbBtPbAtM5cB28o6wHnAsvK1DvgMdAIDuB54E7ACuP6F0JAk9ce0AZCZXwUOTti8BthYljcCF43bflt2PAAsiIjTgXcC92bmwcw8BNzLi0NFktSiyMzpO0UsBe7OzLPK+rOZuWBc+6HMXBgRdwM3ZObXyvZtwHXAKHByZv5Z2f5HwNHM/ItJxlpH5+yB4eHh5Zs3b+75yR05coShoaGe958r1tWdQa3rwMHD7D/a/rhnL5o/Zfugzpd1dadJXatWrdqRmSPT9ZvtPwgTk2zLKba/eGPmBmADwMjISI6OjvZczNjYGE32nyvW1Z1BrevmTVu4cWf7f1Npz2WjU7YP6nxZV3faqKvXdwHtL5d2KN8PlO17gSXj+i0GnpliuySpT3oNgK3AC+/kWQtsGbf98vJuoJXA4czcB3wFeEdELCwv/r6jbJMk9cm0568R8QU61/BPi4i9dN7NcwNwR0RcBTwJXFK63wOcD+wGngOuBMjMgxHxp8DXS7+PZubEF5YlSS2aNgAy893HaFo9Sd8Erj7G49wK3NpVdZKkOeMngSWpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKtUoACLiDyLisYh4NCK+EBEnR8QZEbE9Ih6PiNsj4qTS9+VlfXdpXzobT0CS1JueAyAiFgEfAEYy8yzgBOBS4OPATZm5DDgEXFV2uQo4lJmvBW4q/SRJfdL0EtA84BURMQ94JbAPeBtwZ2nfCFxUlteUdUr76oiIhuNLknoUmdn7zhHXAB8DjgL/DFwDPFB+yycilgBfzsyzIuJR4NzM3Fvavgu8KTO/P+Ex1wHrAIaHh5dv3ry55/qOHDnC0NBQz/vPFevqzqDWdeDgYfYfbX/csxfNn7J9UOfLurrTpK5Vq1btyMyR6frN6+nRgYhYSOe3+jOAZ4G/B86bpOsLCTPZb/svSp/M3ABsABgZGcnR0dFeS2RsbIwm+88V6+rOoNZ186Yt3Liz50OoZ3suG52yfVDny7q600ZdTS4BvR34z8z8Xmb+D3AX8FvAgnJJCGAx8ExZ3gssASjt84GDDcaXJDXQJACeBFZGxCvLtfzVwLeA+4GLS5+1wJayvLWsU9rvyybXnyRJjfQcAJm5nc6LuQ8BO8tjbQCuA66NiN3AqcAtZZdbgFPL9muB9Q3qliQ11OgCZmZeD1w/YfMTwIpJ+v4EuKTJeJKk2eMngSWpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKtUoACJiQUTcGRHfjohdEfHmiHh1RNwbEY+X7wtL34iIT0fE7oh4JCLOmZ2nIEnqRdMzgE8B/5SZvwa8HtgFrAe2ZeYyYFtZBzgPWFa+1gGfaTi2JKmBngMgIl4FvBW4BSAzf5qZzwJrgI2l20bgorK8BrgtOx4AFkTE6T1XLklqJDKztx0j3gBsAL5F57f/HcA1wNOZuWBcv0OZuTAi7gZuyMyvle3bgOsy88EJj7uOzhkCw8PDyzdv3txTfQBHjhxhaGio5/3ninV1Z1DrOnDwMPuPtj/u2YvmT9k+qPNlXd1pUteqVat2ZObIdP3m9fToP9/3HOD9mbk9Ij7Fzy/3TCYm2fai9MnMDXSChZGRkRwdHe25wLGxMZrsP1esqzuDWtfNm7Zw484mh1Bv9lw2OmX7oM6XdXWnjbqavAawF9ibmdvL+p10AmH/C5d2yvcD4/ovGbf/YuCZBuNLkhroOQAy87+ApyLidWXTajqXg7YCa8u2tcCWsrwVuLy8G2glcDgz9/U6viSpmabnr+8HNkXEScATwJV0QuWOiLgKeBK4pPS9Bzgf2A08V/pKkvqkUQBk5sPAZC80rJ6kbwJXNxlPkjR7/CSwJFXKAJCkShkAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlGgdARJwQEd+IiLvL+hkRsT0iHo+I2yPipLL95WV9d2lf2nRsSVLvZuMM4Bpg17j1jwM3ZeYy4BBwVdl+FXAoM18L3FT6SZL6pFEARMRi4ALgc2U9gLcBd5YuG4GLyvKask5pX136S5L6oOkZwCeBDwE/K+unAs9m5vNlfS+wqCwvAp4CKO2HS39JUh9EZva2Y8S7gPMz830RMQr8IXAl8G/lMg8RsQS4JzPPjojHgHdm5t7S9l1gRWb+YMLjrgPWAQwPDy/fvHlzb88MOHLkCENDQz3vP1esqzuDWteBg4fZf7T9cc9eNH/K9kGdL+vqTpO6Vq1atSMzR6brN6+nR+94C3BhRJwPnAy8is4ZwYKImFd+y18MPFP67wWWAHsjYh4wHzg48UEzcwOwAWBkZCRHR0d7LnBsbIwm+88V6+rOoNZ186Yt3LizySHUmz2XjU7ZPqjzZV3daaOuni8BZeaHM3NxZi4FLgXuy8zLgPuBi0u3tcCWsry1rFPa78teTz8kSY3NxecArgOujYjddK7x31K23wKcWrZfC6yfg7ElSTM0K+evmTkGjJXlJ4AVk/T5CXDJbIwnSWrOTwJLUqUMAEmqlAEgSZVq/z1sLdr59GGuWP+l1sfdc8MFrY8pSd3yDECSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSL+m7gUqaXUsb3F33g2c/3/Pdeb3D7tzwDECSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASapUzwEQEUsi4v6I2BURj0XENWX7qyPi3oh4vHxfWLZHRHw6InZHxCMRcc5sPQlJUveanAE8D3wwM38dWAlcHRFnAuuBbZm5DNhW1gHOA5aVr3XAZxqMLUlqqOcAyMx9mflQWf4RsAtYBKwBNpZuG4GLyvIa4LbseABYEBGn91y5JKmRWXkNICKWAm8EtgPDmbkPOiEB/GLptgh4atxue8s2SVIfRGY2e4CIIeBfgY9l5l0R8WxmLhjXfigzF0bEl4A/z8yvle3bgA9l5o4Jj7eOziUihoeHl2/evLnn2g4cPMz+oz3v3rOzF82fsv3IkSMMDQ21VM3MWVd3avz52vn04Z73HX4FPc/XdM+5iUH9+WpS16pVq3Zk5sh0/Rr9QZiIOBH4IrApM+8qm/dHxOmZua9c4jlQtu8FlozbfTHwzMTHzMwNwAaAkZGRHB0d7bm+mzdt4cad7f/Nmz2XjU7ZPjY2RpPnNVesqzs1/nz1+gddoPMHYXqdr+mecxOD+vPVRl1N3gUUwC3Arsz8xLimrcDasrwW2DJu++Xl3UArgcMvXCqSJLWvya8vbwHeA+yMiIfLto8ANwB3RMRVwJPAJaXtHuB8YDfwHHBlg7ElSQ31HADlWn4co3n1JP0TuLrX8SRJs8tPAktSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVKn2P8euObO0wcf0ofNR/V4/6r/nhgsajS2pfZ4BSFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlS3gxOko6h6Q0Wm/j8uafM+RieAUhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIq1XoARMS5EfGdiNgdEevbHl+S1NFqAETECcBfAecBZwLvjogz26xBktTR9hnACmB3Zj6RmT8FNgNrWq5BkkT7AbAIeGrc+t6yTZLUssjM9gaLuAR4Z2a+t6y/B1iRme8f12cdsK6svg74ToMhTwO+32D/uWJd3bGu7lhXd16Kdf1KZr5muk5t3w10L7Bk3Ppi4JnxHTJzA7BhNgaLiAczc2Q2Hms2WVd3rKs71tWdmutq+xLQ14FlEXFGRJwEXApsbbkGSRItnwFk5vMR8XvAV4ATgFsz87E2a5AkdbT+B2Ey8x7gnpaGm5VLSXPAurpjXd2xru5UW1erLwJLkgaHt4KQpEod9wEw3a0lIuLlEXF7ad8eEUsHpK4rIuJ7EfFw+XpvS3XdGhEHIuLRY7RHRHy61P1IRJwzIHWNRsThcfP1xy3VtSQi7o+IXRHxWERcM0mf1udshnW1PmcRcXJE/HtEfLPU9SeT9Gn9mJxhXf06Jk+IiG9ExN2TtM3tXGXmcftF54Xk7wK/CpwEfBM4c0Kf9wGfLcuXArcPSF1XAH/Zhzl7K3AO8Ogx2s8HvgwEsBLYPiB1jQJ392G+TgfOKcu/APzHJP+Wrc/ZDOtqfc7KHAyV5ROB7cDKCX36cUzOpK5+HZPXAn832b/VXM/V8X4GMJNbS6wBNpblO4HVEREDUFdfZOZXgYNTdFkD3JYdDwALIuL0AairLzJzX2Y+VJZ/BOzixZ9eb33OZlhX68ocHCmrJ5aviS80tn5MzrCu1kXEYuAC4HPH6DKnc3W8B8BMbi3x/30y83ngMHDqANQF8NvlksGdEbFkkvZ+GOTbdby5nMJ/OSJ+o+3By+n3G+n89jheX+dsirqgD3NWLmk8DBwA7s3MY85Xi8fkTOqC9o/JTwIfAn52jPY5navjPQAmS8KJqT6TPrNtJmP+I7A0M38T+Bd+nvL91o/5momH6Hy8/fXAzcA/tDl4RAwBXwR+PzN/OLF5kl1ambNp6urLnGXm/2bmG+h80n9FRJw1oUtf5msGdbV6TEbEu4ADmbljqm6TbJu1uTreA2DaW0uM7xMR84D5zP2lhpnc8uIHmfnfZfWvgeVzXNNMzWROW5eZP3zhFD47nyU5MSJOa2PsiDiRzn+ymzLzrkm69GXOpqurn3NWxnwWGAPOndDUj2Ny2rr6cEy+BbgwIvbQuUz8toj42wl95nSujvcAmMmtJbYCa8vyxcB9WV5R6WddE64RX0jnGu4g2ApcXt7ZshI4nJn7+l1URPzSC9c+I2IFnZ/dH7QwbgC3ALsy8xPH6Nb6nM2krn7MWUS8JiIWlOVXAG8Hvj2hW+vH5EzqavuYzMwPZ+bizFxK5/+I+zLzdyZ0m9O5av2TwLMpj3FriYj4KPBgZm6lc5D8TUTsppOclw5IXR+IiAuB50tdV8x1XQAR8QU67w45LSL2AtfTeUGMzPwsnU9pnw/sBp4DrhyQui4GfjcingeOApe2EOTQ+S3tPcDOcv0Y4CPAL4+rrR9zNpO6+jFnpwMbo/PHn14G3JGZd/f7mJxhXX05Jidqc678JLAkVep4vwQkSeqRASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqX+D3H65mSA7fYKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dist = (test_df.diagnosis.value_counts()/len(test_df))*100\n",
    "print('Prediction distribution:')\n",
    "print(dist)\n",
    "test_df.diagnosis.hist()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
